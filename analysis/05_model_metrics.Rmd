---
title: "Models"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(lme4)
library(broom.mixed)
library(lmerTest)
```
#### Read in data, get final dataset for models
Get data and put it in long form 
```{r}
pins::board_register_github(repo = "karinorman/biodivTS_data", branch = "master")

#get metadata
meta <- pins::pin_get("meta", board = "github") %>%
  select(study_id, realm, climate, habitat, protected_area, biome_map)

meta <- pins::pin_get("bt-traitfiltered", board = "github") %>%
  select(study_id, rarefyid, taxa) %>% distinct() %>%
  left_join(meta)

#get metric data
metrics <- pins::pin_get("rarefied-metrics", board = "github") %>%
  # pivot_longer(., cols = setdiff(everything(), one_of("rarefyID", "STUDY_ID", "SamplePool", "SampleN", "num_years", "duration", "startYear", "endYear", "YEAR", "cell", "rarefied", "type", "realm", "climate", "habitat", "biome_map", "taxa", "organisms", "cent_lat", "cent_long", "abundance_type", "BROAD_TYPE", "taxa_mod", "climate_mod")), names_to = "metric", values_to = "value") %>%
  rename_with(tolower, .cols = setdiff(everything(), one_of("rarefyID")))  %>%
  mutate(logvalue = log(value + 1)) %>% # create a column for log transformation that adds the absolute value integer closest to the most negative value to create all positive values
  left_join(meta, c("rarefyID" = "rarefyid"))

## Now the data that actually makes it into the model ##

#get rarefyID's that don't have enough null samples
missing_null <- bind_rows(metrics %>% 
  filter(n_missing_nulls == 1, commplete_null_samps == FALSE),
  metrics %>% filter(n_missing_nulls > 100)) %>%
  pull(rarefyID) %>%
  unique()

model_data <- metrics %>% 
  filter(metric %in% c("SES_FRic", "SES_FEve", "SES_FDiv", "Jaccard_base", "Jaccard_next",
                       "FRic", "FEve", "FDiv", "S"),
         !rarefyID %in% missing_null) %>%
  group_by(metric) %>%
  mutate(study_id =  str_extract(rarefyID, "[^_]+")) %>%
  mutate(year_scaled = scale(year), 
         scale_center = attributes(year_scaled)$`scaled:center`, 
         scale = attributes(year_scaled)$`scaled:scale`) %>%
  ungroup() %>%
  #this study has many low species richness observations, so FD metrics weren't calculated, just drop it
  filter(study_id != 348)

pins::pin(model_data, board = "github")
```

Write out file with all necessary metadata for the supplement.
```{r}
meta %>% 
  filter(study_id %in% model_data$study_id) %>% 
  select(-c(link_id, date_study_added, sample_desc_name)) %>% 
  write_csv(here::here("paper", "study_metadata.csv"))
```


Option to add human footprint data
```{r}
# hfp_data <- pins::pin_get("hfp-data", board = "github")
# 
# model_data_hfp <- model_data %>% 
#   #dplyr::select(rarefyID, year) %>% 
#   #distinct() %>%
#   mutate(hfp_year = case_when(
#     year < 2001 ~ 1993,
#     TRUE ~ 2009
#   ), 
#   hfp_rarefyID = str_replace_all(rarefyID, pattern = "_bird", ""), 
#   hfp_rarefyID = str_replace_all(hfp_rarefyID, pattern = "_mamm", "")) %>%
#   left_join(hfp_data %>% dplyr::select(-STUDY_ID) %>% mutate(year = as.numeric(year)), by = c("hfp_year" = "year", "hfp_rarefyID" = "rarefyID")) %>%
#   dplyr::select(-hfp_year)

```

####Data exploration

Visualize number of years for each timeseries
```{r}
hist_numYears <- metrics %>%
  select(year, rarefyID) %>% 
  distinct() %>% 
  count(rarefyID, name = "num_years") %>% 
  ggplot() +
  geom_histogram(aes(x = num_years),
                 binwidth = 2) +
  scale_x_continuous(name = 'Number of years sampled',
                     breaks = c(2,4,8,16,32,48,64,96)) +
  #scale_y_continuous(breaks = c(0,2500,5000,7500,10000,12500, 15000, 17500, 20000,22500)) +
  labs(tag = 'B',
       y = 'Number of cells') +
  theme_bw() +
  theme(panel.grid.minor.x = element_blank())
```

Visualize distributions of response
```{r}
metrics %>%
  filter(metric %in% c("FRic", "FEve", "FDiv", "FDis", "S", "Jaccard_hind")) %>%
  ggplot() +
  geom_histogram(aes(x = logvalue),
                 bins = 60) +
  facet_wrap(~metric, scales = "free") +
  labs(title = "Distribution of Log values")

metrics %>%
  filter(metric %in% c("FRic", "FEve", "FDiv", "FDis", "S", "SES_FDis", "SES_FDiv", "SES_FEve", "SES_FRic", "Jaccard_hind")) %>%
  ggplot() +
  geom_histogram(aes(x = value),
                 bins = 60) +
  facet_wrap(~metric, scales = "free") +
  labs(title = "Distribution of raw values")
```

####Fit GLMs to overall patterns and data broken down by groups

Convergence problems! See here:
https://joshua-nugent.github.io/allFit/
https://rstudio-pubs-static.s3.amazonaws.com/33653_57fc7b8e5d484c909b615d8633c01d51.html
https://biologyforfun.wordpress.com/2018/04/09/help-i-have-convergence-warnings/

Model checks:
https://debruine.github.io/posts/normality/
performance::check_model()

Function approach to model
```{r}
get_model <- function(metric_name, formula, model_id, data, ...){
  print(c(metric_name, model_id))
  qlmer <- quietly(lmer)
  
  fit <- data %>%
    filter(metric == metric_name) %>%
    mutate(year_scaled = scale(year)) %>%
    qlmer(formula = formula, data = ., ...)
  
  tidy(fit$result) %>%
   mutate(metric = metric_name, model = list(fit$result), model_id = model_id, warning = ifelse(length(fit$warnings) == 0, NA, fit$warnings))
}

#get_model(data = model_data, metric_name = "SES_FRic", "value ~ year_scaled + (year_scaled|study_id/rarefyID) + (year_scaled|climate)", model_id = "base")
```

Create dataframe of models to fit
```{r}
raw_value_mods <- tibble(metric_name = c("SES_FRic", "SES_FEve", "SES_FDiv", "Jaccard_base"), 
                         base = "value ~ year_scaled + (year_scaled|study_id/rarefyID)",
                         taxa = "value ~ year_scaled + (year_scaled|study_id/rarefyID) + (year_scaled|taxa)",
                         realm = "value ~ year_scaled + (year_scaled|study_id/rarefyID) + (year_scaled|realm)",
                         climate = "value ~ year_scaled + (year_scaled|study_id/rarefyID) + (year_scaled|climate)") %>%
  pivot_longer(cols = c(base, taxa, realm, climate), names_to = "model_id", values_to = "formula")

log_value_mods <- tibble(metric_name = c("S"), 
                         base = "logvalue ~ year_scaled + (year_scaled|study_id/rarefyID)",
                         taxa = "logvalue ~ year_scaled + (year_scaled|study_id/rarefyID) + (year_scaled|taxa)",
                         realm = "logvalue ~ year_scaled + (year_scaled|study_id/rarefyID) + (year_scaled|realm)",
                         climate = "logvalue ~ year_scaled + (year_scaled|study_id/rarefyID) + (year_scaled|climate)") %>%
  pivot_longer(cols = c(base, taxa, realm, climate), names_to = "model_id", values_to = "formula")

mods <- bind_rows(raw_value_mods, log_value_mods) %>% 
  select(metric_name, formula, model_id)
```

Fit models
```{r}
#Fit models with base settings
mod_results <- pmap_dfr(mods, get_model, data = model_data)

#get the models that had convergence warnings
bad_models <- mod_results %>%
  select(metric, model_id, warning) %>% 
  filter(!is.na(warning)) %>% 
  distinct() %>%
  left_join(mods, by = c("metric" = "metric_name", "model_id")) 

#Turn of REML for these models 
bad_mod_results <- pmap_dfr(bad_models %>% select(-warning), get_model, data = model_data, REML = FALSE)

#One model still won't converge
bad_models2 <- bad_mod_results %>%
  select(metric, model_id, warning) %>% 
  filter(!is.na(warning)) %>% 
  distinct() %>%
  left_join(mods, by = c("metric" = "metric_name", "model_id")) 

#Fit with a different algorithm
bad_mod_results2 <- pmap_dfr(bad_models2 %>% select(-warning), get_model, data = model_data, control = lmerControl(optimizer = "bobyqa"))

metric_model_table <- bind_rows(mod_results %>% filter(is.na(warning)), 
          bad_mod_results %>% filter(is.na(warning)),
          bad_mod_results2 %>% filter(is.na(warning))) 

usethis::use_data(metric_model_table)
pins::pin(metric_model_table, board = "github")
```

#### Process Model Output

Get study level slope estimates
```{r}
get_study_slopes <- function(metric, model, model_id){
  
  filter_name <- ifelse(model_id == "base", "study_id", model_id)
  
  #get the statistics for the year variable
  year_scaled_coef <- tidy(model) %>%
    filter(term == "year_scaled") 
  
  #get the year estimate
  year_scaled_est <- pull(year_scaled_coef, estimate) 
  
  #get year variation
  year_scaled_var <- year_scaled_coef %>% 
    mutate(var = std.error*std.error) %>%
    pull(var)
  
  #get study level conditional estimates and add to over all estimates
  study_ests <- broom.mixed::tidy(model, effects="ran_vals") %>%
    filter(group == filter_name, term == "year_scaled") %>% 
    select(-c(effect, group, term)) %>%
    rename(cond.std.error = std.error, cond.estimate = estimate) %>%
    mutate(estimate = cond.estimate + year_scaled_est,
      cond.var = cond.std.error*cond.std.error, 
           var = cond.var + year_scaled_var,
           std.error = sqrt(var),
           upr.ci = estimate + (2*std.error),
           lwr.ci = estimate - (2*std.error),
           sig = case_when(
             lwr.ci < 0 & upr.ci > 0 ~ FALSE,
             TRUE ~ TRUE
           ),
      metric = metric,
      model_id = model_id)
}

model_df <- metric_model_table %>%
  select(metric, model, model_id) %>%
  group_by(metric, model_id) %>% 
  slice_head()

#get study level slopes for base, and levels for categorical random effect in non-base models
re_slopes <- pmap_dfr(model_df, get_study_slopes)
pins::pin(re_slopes, board = "github")

#get only study-level slopes for the base models
study_slopes <- re_slopes %>% 
  filter(model_id == "base") %>%
  rename(study_id = level)

# study_slopes_wide <- study_slopes %>%
#   select(study_id, metric, estimate) %>%
#   pivot_wider(names_from = "metric", values_from = "estimate")
```

Summaries of study slopes
```{r}
#First - how many studies had a significant trend for each metric? 
study_slopes %>% 
  filter(sig == TRUE) %>% 
  filter(metric %in% c("SES_FRic", "SES_FEve", "SES_FDiv", "Jaccard_base", "S")) %>% 
  group_by(metric) %>% 
  summarise(n_study_id = n_distinct(study_id)) %>%
  mutate(percent = n_study_id/53) 

#And which studies were they?
study_slopes %>% 
  filter(sig == TRUE) %>% 
  filter(metric %in% c("SES_FRic", "SES_FEve", "SES_FDiv", "Jaccard_base", "S")) %>% 
  select(study_id, metric) %>%
  distinct()

#Which were significant for multiple metrics?
study_slopes %>% 
  filter(sig == TRUE) %>% 
  filter(metric %in% c("SES_FRic", "SES_FEve", "SES_FDiv", "Jaccard_base", "S")) %>% 
  group_by(study_id) %>%
  filter(n() > 1) %>%
  select(study_id, metric)
```

#### Analizing slopes
Is slope a function of timeseries characteristics?
```{r}
library(multcomp)

study_slope_data <- study_slopes %>%
  dplyr::select(study_id, metric, estimate) %>%
  left_join(meta %>% 
              dplyr::select(-rarefyid) %>% 
              distinct() %>%
              mutate(study_id = as.character(study_id))) %>%
  left_join(model_data %>% dplyr::select(study_id, startyear, duration) %>% distinct())


duration_slope <- study_slope_data %>%
  group_nest(metric) %>%
  mutate(model = map(data, ~glm(estimate ~ duration, 
                                 data = .x)),
           coef = map(model, tidy),
           ) %>% 
  dplyr::select(metric, coef) %>%
  unnest(cols = c(coef)) %>%
  mutate(covariate = "duration")

startyear_slope <- study_slope_data %>%
  group_nest(metric) %>%
  mutate(model = map(data, ~glm(estimate ~ startyear, 
                                 data = .x)),
           coef = map(model, tidy),
           ) %>% 
  dplyr::select(metric, coef) %>%
  unnest(cols = c(coef)) %>%
  mutate(covariate = "startyear")

#All model estimates in one dataframe
slope_models <- bind_rows(duration_slope, startyear_slope)
pins::pin(slope_models, board = "github")

# Which predictor variables were significant?
slope_models %>% filter(term != "(Intercept)", p.value < 0.05) %>% View()
```

Pairwise correlation analysis
```{r}
library(psych)
#slope_corr <- corr.test(slopes_wide %>% ungroup() %>% select(starts_with("SES"), "S", "Jaccard_base"))

slope_corr_bootstrap <- corCi(slopes_wide %>% ungroup() %>% select(starts_with("SES"), "S", "Jaccard_base"))
```

Clustering 
```{r}
library(clValid)
scale2 <- function(x, na.rm = FALSE) (x - mean(x, na.rm = na.rm)) / sd(x, na.rm)

scale_slopes <- slopes_wide %>%  
  drop_na() %>%
  select(study_id, starts_with("SES"), S, Jaccard_base) %>%
  column_to_rownames("study_id") %>%
  mutate(across(.fns = scale2)) %>%
  as.matrix()

clmethods <- c("hierarchical","kmeans","pam")
intern <- clValid(scale_slopes, nClust = 2:6, clMethods = clmethods, validation = "internal")
stab <- clValid(scale_slopes, nClust = 2:6, clMethods = clmethods, validation = "stability")

summary(intern)
optimalScores(stab)

#get the hierarchical clustering object
clust <- intern@clusterObjs$hierarchical
cat5 <- cutree(clust, k = 5)
cat3 <- cutree(clust, k = 3)


slopes_clust <- slopes_wide %>% 
  ungroup() %>%
  left_join(as.data.frame(cat3) %>%
  rownames_to_column("study_id")) %>%
  left_join(as.data.frame(cat5) %>%
  rownames_to_column("study_id"))
```

```{r}
slopes_clust %>% 
  gatherpairs( "SES_FDiv", "SES_FEve", "SES_FRic", "S", "Jaccard_base") %>% {
  ggplot(., aes(x = .xvalue, y = .yvalue, color = as.factor(cat5))) +
      geom_point() + 
      #geom_smooth(method = 'lm') +
      facet_wrap(.xkey ~ .ykey, ncol = length(unique(.$.ykey)), scales = 'free', labeller = label_both) +
      scale_color_brewer(type = 'qual')
  }
```

####Figures

Pairwise plots of study-level slopes
```{r}
#pairwise plots using GGally
ggpairs(slopes_wide %>% 
          select(-c("study_id", "FDiv", "FEve", "FRic")))

#pairwise plots using GGPlot
gatherpairs <- function(data, ..., 
                        xkey = '.xkey', xvalue = '.xvalue',
                        ykey = '.ykey', yvalue = '.yvalue',
                        na.rm = FALSE, convert = FALSE, factor_key = FALSE) {
  vars <- quos(...)
  xkey <- enquo(xkey)
  xvalue <- enquo(xvalue)
  ykey <- enquo(ykey)
  yvalue <- enquo(yvalue)

  data %>% {
    cbind(gather(., key = !!xkey, value = !!xvalue, !!!vars,
                 na.rm = na.rm, convert = convert, factor_key = factor_key),
          select(., !!!vars)) 
  } %>% gather(., key = !!ykey, value = !!yvalue, !!!vars,
               na.rm = na.rm, convert = convert, factor_key = factor_key)
}

slopes_wide %>% 
  gatherpairs( "SES_FDiv", "SES_FEve", "SES_FRic", "S") %>% {
  ggplot(., aes(x = .xvalue, y = .yvalue)) +
      geom_point() + 
      #geom_smooth(method = 'lm') +
      facet_wrap(.xkey ~ .ykey, ncol = length(unique(.$.ykey)), scales = 'free', labeller = label_both) +
      scale_color_brewer(type = 'qual')
  }

slopes_wide %>%
  select(-c("FDiv", "FEve", "FRic")) %>%
  pivot_longer(-one_of("study_id", "S"), names_to = "metric") %>%
  ggplot(aes(x = S, y = value)) +
  geom_point() +
  facet_wrap(~metric) +
  xlim(-1.3, 1.3) +
  ylim(-0.6, 0.6) +
    theme_classic() +
    geom_hline(yintercept = 0) +
    geom_vline(xintercept = 0) + 
  ylab("Metric Slope") +
  xlab("Richness Slope")

slopes_wide %>%
  select(-c("FDiv", "FEve", "FRic")) %>%
  pivot_longer(-one_of("study_id", "SES_FRic"), names_to = "metric") %>%
  ggplot(aes(x = SES_FRic, y = value)) +
  geom_point() +
  facet_wrap(~metric) +
  xlim(-.25, .25) +
  ylim(-0.6, 0.6) +
    theme_classic() +
    geom_hline(yintercept = 0) +
    geom_vline(xintercept = 0) + 
  ylab("Metric Slope") +
  xlab("Functional Richness Slope")
```
